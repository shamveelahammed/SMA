```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r, eval=FALSE}
library(twitteR)
library(ROAuth)

# authenticate with Twitter

consumerKey<-	"XXX"
consumerSecret<-"XXX"

accessToken<-"XXX"
accessSecret<-"XXX"

setup_twitter_oauth (consumerKey, consumerSecret, accessToken, accessSecret)  # authenticate

# search for tweets by keyword

tweets<-searchTwitter("muslim", n=1000, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL,
                                resultType=NULL, retryOnRateLimit=120)
# put tweets in a data frame
tweets_muslim<-twListToDF(tweets)

tweets<-searchTwitter("islam", n=1000, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL,
                                resultType=NULL, retryOnRateLimit=120)
# put tweets in a data frame
tweets_islam<-twListToDF(tweets)

tweets<-searchTwitter("arab", n=1000, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL,
                                resultType=NULL, retryOnRateLimit=120)
# put tweets in a data frame
tweets_arab<-twListToDF(tweets)

tweets<-searchTwitter("muhammad", n=1000, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL,
                                resultType=NULL, retryOnRateLimit=120)
# put tweets in a data frame
tweets_muhammad<-twListToDF(tweets)

all_muslim_tweets <- rbind(tweets_muslim, tweets_islam, tweets_arab, tweets_muhammad)
# write out to a CSV file
write.csv(all_muslim_tweets, file="muslim_tweets.csv")

```


```{r, eval=FALSE}
library(twitteR)
library(ROAuth)

# search for tweets by keyword

tweets<-searchTwitter("christian", n=1000, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL,
                                resultType=NULL, retryOnRateLimit=120)
# put tweets in a data frame
tweets_christian<-twListToDF(tweets)

tweets<-searchTwitter("christ", n=1000, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL,
                                resultType=NULL, retryOnRateLimit=120)
# put tweets in a data frame
tweets_christ<-twListToDF(tweets)

tweets<-searchTwitter("catholic", n=1000, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL,
                                resultType=NULL, retryOnRateLimit=120)
# put tweets in a data frame
tweets_catholic<-twListToDF(tweets)

tweets<-searchTwitter("church", n=1000, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, maxID=NULL,
                                resultType=NULL, retryOnRateLimit=120)
# put tweets in a data frame
tweets_church<-twListToDF(tweets)

all_christian_tweets <- rbind(tweets_christian, tweets_christ, tweets_catholic, tweets_church)
# write out to a CSV file
write.csv(all_christian_tweets, file="christian_tweets.csv")

```

# Cleaning

```{r, eval= FALSE, message=FALSE}
library(syuzhet)
library(plotly)
library(tm)
library(wordcloud)

#import your dataset to analyse, 
#ensure it is in the same directory as your code, 
#otherwise you need to add the path
  
tweets <- read.csv("muslim_tweets.csv")
  clean_tweets = tweets$text
  
  print(length(clean_tweets))
  
  #clean_tweets = sapply(tweets, function(x) x$getText())
  # remove retweet entities
  clean_tweets = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', clean_tweets)
  # remove at people
  clean_tweets = gsub('@\\w+', '', clean_tweets)
  # remove punctuation
  clean_tweets = gsub('[[:punct:]]', '', clean_tweets)
  # remove numbers
  clean_tweets = gsub('[[:digit:]]', '', clean_tweets)
  # remove html links
  clean_tweets = gsub('http\\w+', '', clean_tweets)
  # remove unnecessary spaces
  clean_tweets = gsub('[ \t]{2,}', '', clean_tweets)
  clean_tweets = gsub('^\\s+|\\s+$', '', clean_tweets)
  # remove emojis or special characters
  clean_tweets = gsub('<.*>', '', enc2native(clean_tweets))
  
  clean_muslim_tweets = tolower(clean_tweets)

write.csv(clean_muslim_tweets, file="clean_muslim_tweets.csv")

```


```{r, eval= FALSE, message=FALSE}

tweets <- read.csv("christian_tweets.csv")
  clean_tweets = tweets$text
  
  print(length(clean_tweets))
  
  #clean_tweets = sapply(tweets, function(x) x$getText())
  # remove retweet entities
  clean_tweets = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', clean_tweets)
  # remove at people
  clean_tweets = gsub('@\\w+', '', clean_tweets)
  # remove punctuation
  clean_tweets = gsub('[[:punct:]]', '', clean_tweets)
  # remove numbers
  clean_tweets = gsub('[[:digit:]]', '', clean_tweets)
  # remove html links
  clean_tweets = gsub('http\\w+', '', clean_tweets)
  # remove unnecessary spaces
  clean_tweets = gsub('[ \t]{2,}', '', clean_tweets)
  clean_tweets = gsub('^\\s+|\\s+$', '', clean_tweets)
  # remove emojis or special characters
  clean_tweets = gsub('<.*>', '', enc2native(clean_tweets))
  
  clean_christian_tweets = tolower(clean_tweets)

write.csv(clean_christian_tweets, file="clean_christian_tweets.csv")
  #clean_tweets
```

# Emotion Plot

```{r, eval= FALSE, message=FALSE}
  
emotions <- get_nrc_sentiment(clean_christian_tweets)
emo_bar = colSums(emotions)
emo_sum = data.frame(count=emo_bar, emotion=names(emo_bar))
emo_sum$emotion = factor(emo_sum$emotion, levels=emo_sum$emotion[order(emo_sum$count, decreasing = TRUE)])

emo_sum <- emo_sum[1:8,]
emo_sum$percent<-(emo_sum$count/sum(emo_sum$count))*100
  
   #Visualize the emotions from NRC sentiments
plot_ly(emo_sum, x=~emotion, y=~percent, type="bar", color=~emotion) %>%
layout(xaxis=list(title=""),  yaxis = list(title = "Emotion count"),
showlegend=FALSE,title="Distribution of emotion categories") %>%
layout(yaxis = list(ticksuffix = "%"))
``` 

