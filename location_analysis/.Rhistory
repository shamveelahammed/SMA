library(stringr)
library(ggplot2)
#import your dataset to analyse,
tweets <- read.csv("muslim_tweets.csv")
clean_tweets.df = tweets$text
# convert text to lowercase
clean_tweets.df<-tolower(clean_tweets.df)
# get rid of problem characters
clean_tweets.df <- sapply(clean_tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))
clean_tweets.df = gsub("&amp", "", clean_tweets.df)
#remove retweet entities
clean_tweets.df = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', clean_tweets.df)
# remove at people
clean_tweets.df = gsub('@\\w+', '', clean_tweets.df)
# remove punctuation
clean_tweets.df = gsub('[[:punct:]]', '', clean_tweets.df)
# remove numbers
clean_tweets.df = gsub('[[:digit:]]', '', clean_tweets.df)
# remove html links
clean_tweets.df = gsub('http\\w+', '', clean_tweets.df)
# remove unnecessary spaces
clean_tweets.df = gsub('[ \t]{2,}', '', clean_tweets.df)
clean_tweets.df = gsub('^\\s+|\\s+$', '', clean_tweets.df)
# remove emojis or special characters
clean_tweets.df = gsub('<.*>', '', enc2native(clean_tweets.df))
# get rid of unnecessary spaces
clean_tweets.df <- str_replace_all(clean_tweets.df," "," ")
# get rid of URLs - does not work
#clean_tweets.df <- str_replace_all(clean_tweets.df, "http://t.co/[a-z,A-Z,0-9]*{8}","")
# take out the retweet header (there is only one)
clean_tweets.df <- str_replace(clean_tweets.df,"RT @[a-z,A-Z]*: ","")
# get rid of hashtags
clean_tweets.df <- str_replace_all(clean_tweets.df,"#[a-z,A-Z]*","")
# get rid of references to other screen names
clean_tweets.df <- str_replace_all(clean_tweets.df,"@[a-z,A-Z]*","")
#enable to view
#View(clean_tweets.df)
#store in csv
write.csv(clean_tweets.df, file="clean_muslim_tweets.csv")
# run this block to install these packages if you do not have them installed yet
install.packages("wordcloud")
# load the libraries
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#import dataset and get the text column to analyse
tweets <- read.csv("muslim_tweets.csv")
clean_tweets.df = tweets$text
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(clean_tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=100, col=rainbow(50),min.freq = 5,  scale=c(2.0,0.3))
# for knitting the document and enabling the include_graphics function
library(knitr)
library(png)
# for determining the dimensions of PNG files
include_graphics("wordcloud.png")
# load the libraries
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#import dataset and get the text column to analyse
tweets <- read.csv("muslim_tweets.csv")
clean_tweets.df = tweets$text
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(clean_tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=500, col=rainbow(50),min.freq = 10,  scale=c(2.0,0.3))
# wordcloud(christian_corpus,min.freq = 10,colors=brewer.pal(8, "Dark2"),random.color = TRUE,max.words = 500)
# for knitting the document and enabling the include_graphics function
library(knitr)
library(png)
# for determining the dimensions of PNG files
include_graphics("wordcloud.png")
install.packages("scales")
install.packages("scales")
#loading the library
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)
#import dataset and get the text column to analyse
tweets <- read.csv("muslim_tweets.csv")
clean_tweets.df = tweets$text
#Reading the Lexicon positive and negative words
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")
#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
# Parameters
# sentences: vector of text to score
# pos.words: vector of words of postive sentiment
# neg.words: vector of words of negative sentiment
# .progress: passed to laply() to control of progress bar
# create simple array of scores with laply
scores <- laply(sentences,
function(sentence, pos.words, neg.words)
{
# remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
# remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
# remove digits
sentence <- gsub('\\d+', '', sentence)
#convert to lower
sentence <- tolower(sentence)
# split sentence into words with str_split (stringr package)
word.list <- str_split(sentence, "\\s+")
words <- unlist(word.list)
# compare words to the dictionaries of positive & negative terms
pos.matches <- match(words, pos)
neg.matches <- match(words, neg)
# get the position of the matched term or NA
# we just want a TRUE/FALSE
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
# final score
score <- sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
# data frame with scores for each sentence
scores.df <- data.frame(text=sentences, score=scores)
return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(clean_tweets.df, pos.txt, neg.txt, .progress='text')
View(scores_twitter)
#Summary of the sentiment scores
summary(scores_twitter)
scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negtive', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))
View(scores_twitter)
#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")
#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) +
scale_y_continuous(labels = percent)+labs(y="Score")+
theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1
#loading the library
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)
#import dataset and get the text column to analyse
tweets <- read.csv("muslim_tweets.csv")
clean_tweets.df = tweets$text
#Reading the Lexicon positive and negative words
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")
#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
# Parameters
# sentences: vector of text to score
# pos.words: vector of words of postive sentiment
# neg.words: vector of words of negative sentiment
# .progress: passed to laply() to control of progress bar
# create simple array of scores with laply
scores <- laply(sentences,
function(sentence, pos.words, neg.words)
{
# remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
# remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
# remove digits
sentence <- gsub('\\d+', '', sentence)
#convert to lower
sentence <- tolower(sentence)
# split sentence into words with str_split (stringr package)
word.list <- str_split(sentence, "\\s+")
words <- unlist(word.list)
# compare words to the dictionaries of positive & negative terms
pos.matches <- match(words, pos)
neg.matches <- match(words, neg)
# get the position of the matched term or NA
# we just want a TRUE/FALSE
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
# final score
score <- sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
# data frame with scores for each sentence
scores.df <- data.frame(text=sentences, score=scores)
return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(clean_tweets.df, pos.txt, neg.txt, .progress='text')
#View(scores_twitter)
#Summary of the sentiment scores
summary(scores_twitter)
scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negative', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))
#View(scores_twitter)
#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")
#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) +
scale_y_continuous(labels = percent)+labs(y="Score")+
theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1
#loading the library
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)
#import dataset and get the text column to analyse
tweets <- read.csv("muslim_tweets.csv")
clean_tweets.df = tweets$text
#Reading the Lexicon positive and negative words
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")
#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
# Parameters
# sentences: vector of text to score
# pos.words: vector of words of postive sentiment
# neg.words: vector of words of negative sentiment
# .progress: passed to laply() to control of progress bar
# create simple array of scores with laply
scores <- laply(sentences,
function(sentence, pos.words, neg.words)
{
# remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
# remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
# remove digits
sentence <- gsub('\\d+', '', sentence)
#convert to lower
sentence <- tolower(sentence)
# split sentence into words with str_split (stringr package)
word.list <- str_split(sentence, "\\s+")
words <- unlist(word.list)
# compare words to the dictionaries of positive & negative terms
pos.matches <- match(words, pos)
neg.matches <- match(words, neg)
# get the position of the matched term or NA
# we just want a TRUE/FALSE
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
# final score
score <- sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
# data frame with scores for each sentence
scores.df <- data.frame(text=sentences, score=scores)
return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(clean_tweets.df, pos.txt, neg.txt, .progress='text')
#View(scores_twitter)
#Summary of the sentiment scores
summary(scores_twitter)
scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negative', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))
View(clean_tweets.df)
View(scores_twitter)
#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")
#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) +
scale_y_continuous(labels = percent)+labs(y="Score")+
theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1
library(twitteR)
library(ROAuth)
# Parameter settings
n<-1000
lang<-"lang:en"
since<-NULL #YYYY-MM-DD #in the past seven days only
until<-NULL  #YYYY-MM-DD #if more than that you will get error : tweets were requested but the API can only return 0
locale<-NULL
geocode<-NULL
sinceID<-NULL
maxID<-NULL
resultType<-"mixed"
retryOnRateLimit<-120
# search for tweets by keyword 'muslim'
keyword<-"muslim"
tweets<-searchTwitter(keyword, n, lang, since, until, locale, geocode, sinceID, maxID,
resultType, retryOnRateLimit)
library(twitteR)
library(ROAuth)
# authenticate with Twitter
# DON'T UPLOAD THE KEYS ON GITHUB, AS THIS IS PUBLIC REPO
consumerKey<-	"G51KUQTqdYzMWk0XBoHpGPah2"
consumerSecret<-"9ILQpHv5E5QElY8bfRzqy9Z3b8WIeOctBC4vlX2CplPnPaUgUF"
accessToken<-"1844121-FnMyAIkRngSjqEsniQsTOQvyIYkGQWvKHGXJLlkMHJ"
accessSecret<-"A9hLM7MSfTmIxlN5fRKfQZmeezn0FamNXKWqyzeeJVJAN"
setup_twitter_oauth (consumerKey, consumerSecret, accessToken, accessSecret)  # authenticate
library(twitteR)
library(ROAuth)
# Parameter settings
n<-1000
lang<-"lang:en"
since<-NULL #YYYY-MM-DD #in the past seven days only
until<-NULL  #YYYY-MM-DD #if more than that you will get error : tweets were requested but the API can only return 0
locale<-NULL
geocode<-NULL
sinceID<-NULL
maxID<-NULL
resultType<-"mixed"
retryOnRateLimit<-120
# search for tweets by keyword 'muslim'
keyword<-"muslim"
tweets<-searchTwitter(keyword, n, lang, since, until, locale, geocode, sinceID, maxID,
resultType, retryOnRateLimit)
# put tweets in a data frame
tweets_muslim<-twListToDF(tweets)
library(twitteR)
library(ROAuth)
# Parameter settings
n<-1000
lang<-"en"
since<-NULL #YYYY-MM-DD #in the past seven days only
until<-NULL  #YYYY-MM-DD #if more than that you will get error : tweets were requested but the API can only return 0
locale<-NULL
geocode<-NULL
sinceID<-NULL
maxID<-NULL
resultType<-"mixed"
retryOnRateLimit<-120
# search for tweets by keyword 'muslim'
keyword<-"muslim"
tweets<-searchTwitter(keyword, n, lang, since, until, locale, geocode, sinceID, maxID,
resultType, retryOnRateLimit)
# put tweets in a data frame
tweets_muslim<-twListToDF(tweets)
# search for tweets by keyword 'islam'
keyword<-"islam"
tweets<-searchTwitter(keyword, n, lang, since, until, locale, geocode, sinceID, maxID,
resultType, retryOnRateLimit)
# put tweets in a data frame
tweets_islam<-twListToDF(tweets)
# search for tweets by keyword 'arab'
keyword<-"arab"
tweets<-searchTwitter(keyword, n, lang, since, until, locale, geocode, sinceID, maxID,
resultType, retryOnRateLimit)
# put tweets in a data frame
tweets_arab<-twListToDF(tweets)
# search for tweets by keyword 'muhammad'
keyword<-"arab"
tweets<-searchTwitter(keyword, n, lang, since, until, locale, geocode, sinceID, maxID,
resultType, retryOnRateLimit)
# put tweets in a data frame
tweets_muhammad<-twListToDF(tweets)
# search for tweets by keyword
keyword<-"arab"
tweets<-searchTwitter(keyword, n, lang, since, until, locale, geocode, sinceID, maxID,
resultType, retryOnRateLimit)
# put tweets in a data frame
tweets_arab<-twListToDF(tweets)
#combine tweet dataframes
all_muslim_tweets <- rbind(tweets_muslim, tweets_islam, tweets_arab, tweets_muhammad)
# write out to a CSV file
write.csv(all_muslim_tweets, file="muslim_tweets.csv")
# load the libraries
library(syuzhet)
library(plotly)
library(tm)
library(plyr)
library(stringr)
library(ggplot2)
#import your dataset to analyse,
tweets <- read.csv("muslim_tweets.csv")
clean_tweets.df = tweets$text
# convert text to lowercase
clean_tweets.df<-tolower(clean_tweets.df)
# get rid of problem characters
clean_tweets.df <- sapply(clean_tweets.df,function(row) iconv(row, "latin1", "ASCII", sub=""))
clean_tweets.df = gsub("&amp", "", clean_tweets.df)
#remove retweet entities
clean_tweets.df = gsub('(RT|via)((?:\\b\\W*@\\w+)+)', '', clean_tweets.df)
# remove at people
clean_tweets.df = gsub('@\\w+', '', clean_tweets.df)
# remove punctuation
clean_tweets.df = gsub('[[:punct:]]', '', clean_tweets.df)
# remove numbers
clean_tweets.df = gsub('[[:digit:]]', '', clean_tweets.df)
# remove html links
clean_tweets.df = gsub('http\\w+', '', clean_tweets.df)
# remove unnecessary spaces
clean_tweets.df = gsub('[ \t]{2,}', '', clean_tweets.df)
clean_tweets.df = gsub('^\\s+|\\s+$', '', clean_tweets.df)
# remove emojis or special characters
clean_tweets.df = gsub('<.*>', '', enc2native(clean_tweets.df))
# get rid of unnecessary spaces
clean_tweets.df <- str_replace_all(clean_tweets.df," "," ")
# get rid of URLs - does not work
#clean_tweets.df <- str_replace_all(clean_tweets.df, "http://t.co/[a-z,A-Z,0-9]*{8}","")
# take out the retweet header (there is only one)
clean_tweets.df <- str_replace(clean_tweets.df,"RT @[a-z,A-Z]*: ","")
# get rid of hashtags
clean_tweets.df <- str_replace_all(clean_tweets.df,"#[a-z,A-Z]*","")
# get rid of references to other screen names
clean_tweets.df <- str_replace_all(clean_tweets.df,"@[a-z,A-Z]*","")
#enable to view
#View(clean_tweets.df)
#store in csv
write.csv(clean_tweets.df, file="clean_muslim_tweets.csv")
# load the libraries
library(plyr)
library(stringr)
library(tm)
library(wordcloud)
#import dataset and get the text column to analyse
tweets <- read.csv("muslim_tweets.csv")
clean_tweets.df = tweets$text
# corpus will hold a collection of text documents
tweet_corpus <- Corpus(VectorSource(clean_tweets.df))
tweet_corpus
inspect(tweet_corpus[1])
# clean text
tweet_clean <- tm_map(tweet_corpus, removePunctuation)
tweet_clean <- tm_map(tweet_clean, removeWords, stopwords("english"))
tweet_clean <- tm_map(tweet_clean, removeNumbers)
tweet_clean <- tm_map(tweet_clean, stripWhitespace)
wordcloud(tweet_clean, random.order=0.5,max.words=500, col=rainbow(50),min.freq = 10,  scale=c(2.0,0.3))
# wordcloud(christian_corpus,min.freq = 10,colors=brewer.pal(8, "Dark2"),random.color = TRUE,max.words = 500)
# for knitting the document and enabling the include_graphics function
library(knitr)
library(png)
# for determining the dimensions of PNG files
include_graphics("wordcloud2.png")
#loading the library
library(plyr)
library(stringr)
library(ggplot2)
library(tm)
library(scales)
#import dataset and get the text column to analyse
tweets <- read.csv("muslim_tweets.csv")
clean_tweets.df = tweets$text
#Reading the Lexicon positive and negative words
pos <- readLines("positive_words.txt")
neg <- readLines("negative_words.txt")
#function to calculate sentiment score
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
# Parameters
# sentences: vector of text to score
# pos.words: vector of words of postive sentiment
# neg.words: vector of words of negative sentiment
# .progress: passed to laply() to control of progress bar
# create simple array of scores with laply
scores <- laply(sentences,
function(sentence, pos.words, neg.words)
{
# remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
# remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
# remove digits
sentence <- gsub('\\d+', '', sentence)
#convert to lower
sentence <- tolower(sentence)
# split sentence into words with str_split (stringr package)
word.list <- str_split(sentence, "\\s+")
words <- unlist(word.list)
# compare words to the dictionaries of positive & negative terms
pos.matches <- match(words, pos)
neg.matches <- match(words, neg)
# get the position of the matched term or NA
# we just want a TRUE/FALSE
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
# final score
score <- sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
# data frame with scores for each sentence
scores.df <- data.frame(text=sentences, score=scores)
return(scores.df)
}
#sentiment score
scores_twitter <- score.sentiment(clean_tweets.df, pos.txt, neg.txt, .progress='text')
#View(scores_twitter)
#Summary of the sentiment scores
summary(scores_twitter)
scores_twitter$score_chr <- ifelse(scores_twitter$score < 0,'Negative', ifelse(scores_twitter$score > 0, 'Positive', 'Neutral'))
View(clean_tweets.df)
View(scores_twitter)
#Convert score_chr to factor for visualizations
scores_twitter$score_chr <- as.factor(scores_twitter$score_chr)
names(scores_twitter)[3]<-paste("Sentiment")
#plot to show number of negative, positive and neutral comments
Viz1 <- ggplot(scores_twitter, aes(x=Sentiment, fill=Sentiment))+ geom_bar(aes(y = (..count..)/sum(..count..))) +
scale_y_continuous(labels = percent)+labs(y="Score")+
theme(text =element_text(size=15))+theme(axis.text = element_text(size=15))+ theme(legend.position="none")+ coord_cartesian(ylim=c(0,0.6)) + scale_fill_manual(values=c("firebrick1", "grey50", "limeGREEN"))
Viz1
